ould you rip the function defs and map them to “exact PTX code with args” for an IR/DB?

Yes — as a starting point, it’s a pretty good “machine-readable-ish” catalog. But you’ll want to be careful about what information you do and don’t get from the signatures alone.

What you can extract cleanly from CCCL signatures

From the function signature and name you can usually derive:

Opcode family: tcgen05.mma

Required qualifiers: cta_group::{1|2}, kind::{f16|tf32|...}, optional block_scale{...}, optional collector_usage, optional ashift

Operand list / kinds: [d_tmem], a_desc/[a_tmem], b_desc, idesc, disable_output_lane, predicates/immediates, optional scale operands, etc.

Variant constraints: e.g., disable_output_lane array size depends on cta_group (your PTX spec text calls that out), and CCCL splits variants accordingly. (You’ll also see CCCL only expose some combinations depending on toolchain/arch support.) 
NVIDIA Docs
+1

That’s already enough to build a database row like:

tcgen05.mma | qualifiers={cta_group=2, kind=f16} | operands=[d_tmem, a_desc, b_desc, idesc, disable_mask[8], enable_input_d, scale_input_d] | arch>=sm_100a | ptx>=8.6

What you won’t get “for free” from CCCL signatures

You still need to source (from PTX ISA text and/or CCCL metadata):

Exact legality rules that don’t show up in the signature:

“All tcgen05 instructions in a kernel must use the same .cta_group” (kernel-level constraint) 
NVIDIA Docs

shape-specific constraints (e.g., .ashift only legal for some M) 
NVIDIA Docs

the huge combinatorial legality matrix in the PTX tables (e.g., which .scale_vectorsize works with which .kind and K) 
NVIDIA Docs

Architecture / PTX version gating (CCCL often compiles conditionally using internal macros for PTX ISA version / arch). The CCCL docs explicitly track these macros. 
nvidia.github.io

The rendering: turning the structured variant into the exact PTX string formatting (qualifier ordering, braces formatting, operand formatting rules like { disable-output-lane }, etc.). You can write this deterministically, but it’s not fully determined by the C++ signature alone.

A “thoughtful” take for your IR/DB design

If your goal is “NN emits arbitrary PTX decisions with no loss of generality”, I’d treat CCCL as one input source, not the source of truth:

Source of truth for semantics + legality: PTX ISA spec. 
NVIDIA Docs

Great practical catalog of callable variants + operand typing: CCCL generated PTX wrappers (what you’re looking at). 
Arch Linux
+2
nvidia.github.io
+2

Concretely, I’d build the DB with these layers:

Instruction core: name, semantics, async/sync, side effects

Variant: qualifiers (including nested ones like block_scale{scale_vec}), operand schema, immediate schema

Constraints:

per-operand constraints (types, sizes)

per-variant constraints (K/M/N restrictions)

kernel-level constraints (like the .cta_group uniformity rule)

Targeting: min PTX ISA version, min SM architecture, plus “family variants” if relevant

Renderer: canonical PTX pretty-printer so you can round-trip exactly

Then: use CCCL signatures as a way to enumerate variants and operand schemas, but always reconcile against the PTX ISA spec tables/notes so your DB doesn’t silently inherit any CCCL omissions or toolchain limitations. 
NVIDIA Docs
+1

Bottom line

Your read is close: the CCCL page is documenting a header-level API, but it’s not just “theoretical”; it’s meant to compile into real PTX instruction emission. 
Arch Linux
+2
wmaxey.github.io
+2

Yes, you can rip those function defs/signatures to seed an IR/DB mapping to PTX—just don’t let that be your only legality source. Use PTX ISA to fill in the constraints and the complete variant matrix. 
NVIDIA Docs

assume i want an ir / db that contains the full list of ptx instructions, up until blackwell, with no loss of generality,i.e the nn can do everything a human can at the ptx level, and as much legality of ops as possible , ideally full legality although im not sure if theres any ground truth that can enable both of these , and i prioritize generality over legality, compare our options of 1. working off the ptx isa, stripping it of text, or the other libcudaxx i listed, from what it seems ptx isa would be more difficult to parse, and result in more errors, although it may contain more generality, go over how you would approach this, and think deeply about it

If the bar is: “full PTX through Blackwell, no loss of generality, NN can do anything a human can at PTX level”, then the key design decision is: what is your ground-truth corpus of variants and constraints, and what do you do when the ground truth is incomplete/ambiguous.

You’re comparing two starting points:

PTX ISA docs (the official spec)

CCCL / libcu++ PTX wrappers (the generated __ptx/instructions/generated/*.h catalog you found)

They’re complementary, but they optimize for different things.

What “no loss of generality” really means here

To be “no loss”, your IR needs to represent at least:

Exact mnemonic + qualifier chain (including nested brace qualifiers like block_scale{.scale_vec::...})

Operand forms (e.g., a-desc vs [a-tmem], vector operands like disable-output-lane, predicates, immediates)

Types and address spaces where PTX makes them part of the instruction surface

Version/arch gating (PTX ISA version + sm_XX constraints)

Legality constraints:

local (operand type/range rules)

variant-level (tables like “valid combinations of kind × shape × scale_vectorsize”)

program-level (e.g., tcgen05 “all tcgen05 in kernel must use same .cta_group”)

You already noticed the hard part: general + fully legal requires a formalization of those tables and cross-cutting rules. The official ISA is the closest thing to “ground truth” for that.

Option A: Start from PTX ISA docs
Pros

Max generality: it’s the definition of PTX. It includes variants that wrappers may omit or lag.

Max legality signal: the ISA text + tables are where the real constraints live. That includes weird rules that don’t appear in function signatures (kernel-level constraints, shape restrictions, “only valid when X”, default behaviors, aliases, etc.).

Version chronology: ISA notes (“introduced in PTX 8.6”, etc.) and target notes are explicit. 
NVIDIA Docs
+2
NVIDIA Docs
+2

Cons

Parsing is genuinely annoying:

syntax blocks are semi-structured, but the rest is prose and tables

qualifier/operand rules are scattered (“see Table 56”, “see Matrix Descriptors”, “see Issue Granularity”)

PDFs/HTML aren’t designed as a machine-readable spec

Error risk is real: if you “strip text”, you can accidentally drop constraints that matter, or misinterpret a table entry.

What I’d do if using PTX ISA as primary

Treat ISA as the canonical semantics + legality layer, not as something you must perfectly parse in one pass.

Build a pipeline that extracts structured data conservatively and leaves “unknown/uncertain” as first-class.

Concretely:

Parse instruction pages into a “spec graph”

For each instruction section, extract:

mnemonic root (tcgen05.mma)

enumerate syntax forms (each Syntax block line becomes a candidate variant)

collect “Qualifier sets” and “Operand descriptions”

collect “PTX ISA Notes” and “Target ISA Notes”

Keep back-references: “see Table X”, “see section Y”.

Extract tables as constraints

Tables are where the legality lives. Don’t “strip” them—turn them into constraint objects.

If table extraction is imperfect, store the raw table text alongside your parsed structure so you can audit.

Represent uncertainty

You want “no loss of generality”, so when legality is unclear, keep the variant but mark constraints as unknown rather than deleting.

This approach is slower to implement, but it’s the only one that can plausibly get you both breadth and legality.

Option B: Start from CCCL/libcu++ PTX wrappers (generated/*.h)

The big practical discovery here is: CCCL ships a generated header per instruction (you even found tcgen05_mma.h in the CUDA include tree). 
Arch Linux

That means there exists an internal instruction DB somewhere in NVIDIA/CCCL’s build pipeline—CCCL is already the “structured catalog” you wish the ISA were.

Pros

Much easier to mechanize: C++ signatures already encode operand arity, operand kinds, and often which qualifiers exist as template parameters.

Variant enumeration is “clean”: overload sets approximate a list of legal spellings.

Immediate encoding is explicit: types like n32_t<N> force immediate-ness in a way the ISA text doesn’t. (Great for IR.)

You get callable subsets: whatever CCCL exposes is likely to work with toolchains.

Cons (this is the killer for your stated priority)

Not guaranteed complete:

wrappers can lag new PTX

wrappers can omit niche variants

wrappers may reflect “what NVIDIA chose to expose”, not “everything PTX can express”

Legality is partial:

signatures capture some constraints (e.g., operand count, some qualifier presence)

but many legality rules are not expressible in the type system (shape tables, inter-qualifier constraints, kernel-level constraints)

You inherit CCCL’s worldview: if CCCL chose not to model an operand form (or models it differently), you might lose generality unless you cross-check with ISA.

So: CCCL is an excellent index, but a risky sole ground truth if you insist on “NN can do everything a human can”.

The approach I’d actually take: hybrid, with a clear “truth hierarchy”

Given your priority order (generality > legality, but want as much legality as possible), I’d do this:

1) Use CCCL to bootstrap the instruction catalog (high precision, fast)

Parse generated/*.h to enumerate:

instruction names

variant axes (kind, cta_group, cache ops, etc.)

operand schemas

which operands are immediates vs regs vs mem

This gives you a structured DB quickly and with low parse error rate. 
Arch Linux
+1

2) Use PTX ISA as the “completeness + legality augmenter” (ground truth)

For each instruction root found in CCCL, link it to its ISA section.

For any instruction present in ISA but absent from CCCL, add it from ISA as “unwrapped”.

Pull in:

PTX version intro

SM gating

all the weird constraints (tables + notes)

Treat ISA as canonical when there’s a discrepancy.

This gets you the best of both worlds:

CCCL gives you a clean skeleton.

ISA fills in “the rest” and upgrades legality.

3) Build the IR around rendering + verification, not just parsing

To guarantee “no loss of generality” in practice, your IR must be able to:

Render canonical PTX text from the IR (pretty-printer with stable ordering)

Round-trip: PTX text → IR → PTX text (semantics-preserving)

Optionally validate by running a compiler stage:

feed PTX to NVIDIA’s PTX compiler APIs and see what errors you get (this becomes an empirical legality oracle, even if not complete). 
NVIDIA Docs

This is important: even if your legality layer is imperfect, you can attach empirical constraints (“fails with error X on sm_110”) and iteratively refine.

4) Make “legality” a graded system (so you don’t trade away generality)

Instead of a boolean legal/illegal, store:

syntax_valid (can we render it correctly)

type_valid (operand types match expected classes)

constraint_valid (tables/notes satisfied)

toolchain_valid (ptx compiler accepts for target sm/ptx version)

You can prioritize generality while still accumulating legality over time.

Why parsing PTX ISA “stripping text” is the wrong mental model

If you strip the prose, you often lose the very constraints you care about:

alias rules (.block16 aliases)

default behaviors (“if not specified defaults to …”)

cross-instruction rules (“all tcgen05 instructions must use same .cta_group”) — these aren’t in syntax blocks

So I would not aim to “strip text”; I’d aim to structure the text:

keep the prose as linked constraint nodes

progressively annotate into machine rules

This is how you avoid silent loss.

Summary recommendation

If your goal is “full PTX through Blackwell, no loss of generality”:

Use CCCL/libcu++ generated headers as your index / variant enumerator (fast, low-error, structured). 
Arch Linux

Use PTX ISA as your canonical completeness + legality layer (tables + notes + versioning). 
NVIDIA Docs
+1

Add an empirical legality oracle via NVIDIA’s PTX compiler APIs to validate variants against specific targets and harvest errors into your DB. 
NVIDIA Docs

Represent uncertainty explicitly so you never delete expressivity just because legality is unclear.

If you want, next I can sketch a concrete DB schema (tables/entities) that supports:

qualifier chains

operand forms

constraints (declarative + “empirical toolchain”)

per-arch gating

round-trip rendering/parsing hooks